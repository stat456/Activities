---
title: "Activity 5: key"
format: pdf
editor: source
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: FALSE
library(tidyverse)
```

### Week 4 Recap: Conditional Probability and Bayes Rule

-   Marginal, Joint, Conditional Probability

-   Bayes Rule

-   Bayes Rule with data

------------------------------------------------------------------------

### Week 5 Overview: Binomial Probability Exact Analysis

-   Bayes Rule with data

-   "Calculus"

-   Bayesian Data Analysis with binary data

------------------------------------------------------------------------

Solve the following integrals, and write a short summary of why your results are valid.

1. $\int \frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)} \theta^{(a-1)} (1- \theta)^{(b-1)} d\theta$ = 1

2. $\int \theta^{(a-1)} (1- \theta)^{(b-1)} d\theta$ = $\frac{\Gamma(a+b) }{\Gamma(a)\Gamma(b)}$

3. $\int \theta^{(a-1)} (1-\theta)^{(b-1)} \theta^z (1-\theta)^{(N-z)} d\theta$ = $\frac{\Gamma(a+b+N)}{\Gamma(a+z) \Gamma(N-z+b)}$

The law of total probability (and a little arithmetic) makes our lives really easy here.

------------------------------------------------------------------------

Assume you are interested in estimating $\theta$, a probability of an event occurring. You have placed a beta prior distribution on $\theta$ with parameters $a$ and $b$. You've also collected $N$ independent trials and observed $z$ successes. This results in a posterior distribution ($\theta|N,z$) that is beta($a + z, b + N - z).$

Consider the mean of this posterior distribution $\frac{z+a}{N+a+b}$, which can be rewritten as below.

\begin{eqnarray}
\frac{z+a}{N+a+b} &=& \frac{z}{N+a+b} + \frac{a}{N+a+b} \\
&=&\frac{N}{N}\frac{z}{N+a+b} + \frac{a+b}{a+b}\frac{a}{N+a+b}\\
&=&\frac{z}{N}\frac{N}{N+a+b} + \frac{a}{a+b}\frac{a+b}{N+a+b}\\
\end{eqnarray}

Discuss how the posterior mean is a weighted average of the data mean and the prior mean.

What are the weights for each component?

*The posterior mean is a weighted average of the data mean ($\frac{z}{N}$) and the prior mean ($\frac{a}{a+b}$), where the weights are ($\frac{N}{N+a+b}$) for the data piece and ($\frac{a+b}{N+a+b}$) for the prior.*

------------------------------------------------------------------------

from DBDA Exercise 6.4. 

#### a. 
[Purpose: To explore an unusual prior and learn about the beta distribution in the process.] Suppose we have a coin that we know comes from a magic-trick store, and therefore we believe that the coin is strongly biased either usually to come up heads or usually to come up tails, but we don’t know which. Express this belief as a beta prior. (Hint: See Figure 6.1, upper-left panel.) 
*A beta(.1,.1) prior would have this shape. You could also use a mixture of beta distributions.*

#### b.
Now we flip the coin 5 times and it comes up heads in 4 of the 5 flips. What is the posterior distribution? 

*The posterior distribution is Beta(4.1, 1.1)*


#### c.
Create a plot of your posterior distribution and create a line to denote the posterior mean.

```{r}

prob_seq <- seq(0, 1, length.out = 100)

tibble(density = dbeta(prob_seq, 4.1, 1.1),
       prob_seq = prob_seq) %>%
  ggplot(aes(y = density, x = prob_seq)) +
  geom_line() + theme_bw() + xlab ('Heads probability')+
  geom_vline(xintercept = 4.1 / 5.2, color = 'red')
```


------------------------------------------------------------------------

from DBDA Exercise 6.5. 

[Purpose: To get hands on experience with the goal of predicting the next datum, and to see how the prior influences that prediction.]

#### a.
Suppose you have a coin that you know is minted by the government and has not been tampered with. Therefore you have a strong prior belief that the coin is fair. _Say, Beta(10,10)_ You flip the coin 10 times and get 9 heads. What is your predicted probability of heads for the 11th flip (as a 95% interval)? Explain your answer carefully; justify this choice of prior.

*This prior corresponds to the equivalent of 20 coin flips, with equal probability of heads or tails. The posterior distribution would be Beta(19,11) and would have an 95 % interval of (`r round(qbeta(.025,19,11),2)`, `r round(qbeta(.975,19,11),2)`).*

#### b.
Now you have a different coin, this one made of some strange material and marked (in fine print) “Patent Pending, International Magic, Inc.” _You opt for an uniformative prior (Beta(1,1)). You flip the coin 10 times and get 9 heads. What is your predicted probability of heads for the 11th flip  (as a 95% interval)? Explain your answer carefully; justify this choice of prior.

*This prior corresponds to the equivalent of 2 coin flips, with equal probability of heads or tails. The posterior distribution would be Beta(10,2) and would have an 95 % interval of (`r round(qbeta(.025,10,2),2)`, `r round(qbeta(.975,10,2),2)`).*

------------------------------------------------------------------------

Use a dataset containing homes in the Seattle, WA area [http://www.math.montana.edu/ahoegh/teaching/stat408/datasets/SeattleHousing.csv](http://www.math.montana.edu/ahoegh/teaching/stat408/datasets/SeattleHousing.csv) for this question.


Estimate the posterior distribution for the probability that houses in Seattle have more than 3 bedrooms.
```{r}
library(tidyverse)
seattle <- read_csv('http://www.math.montana.edu/ahoegh/teaching/stat408/datasets/SeattleHousing.csv') %>% 
  mutate(more_than3beds = bedrooms > 3)

z <- sum(seattle$more_than3beds)
N <- nrow(seattle)
```

*With a uniform prior, this results in a Beta distribution with a = `r 1 + z` and b = `r 1 + N -z`*

------------------------------------------------------------------------

### 6. Monte Carlo Techniques

We have previously used simulation, in the form of Monte Carlo techniques, to estimate probabilities of outcomes. Soon we will use Monte Carlo techniques to assist with summarizing posterior distributions.

For the following distributions:

1. calculate an exact mean and 95% interval (a quantile based-approach is fine)
2. simulate 1000 samples from the distribution, then calculate an approximate mean and 95% interval 
3. simulate 10000 samples from the distribution, then calculate an approximate mean and 95% interval 
4. simulate 100,000 samples from the distribution, then calculate an approximate mean and 95% interval 

### a
Beta(1,1)

```{r}
sims2a <- rbeta(1000, 1, 1)
sims3a <- rbeta(10000, 1, 1)
sims4a <- rbeta(100000, 1, 1)
```


1. Mean = .5, interval = (.025, .975)

2. Mean = `r round(mean(sims2a),3)`, interval = (`r round(quantile(sims2a, probs = c(.025, .975)),3)`)

3. Mean = `r round(mean(sims3a),3)`, interval = (`r round(quantile(sims3a, probs = c(.025, .975)),3)`)

4. Mean = `r round(mean(sims4a),3)`, interval = (`r round(quantile(sims4a, probs = c(.025, .975)),3)`)

### b
Beta(10,1)

```{r}
sims2b <- rbeta(1000, 10, 1)
sims3b <- rbeta(10000, 10, 1)
sims4b <- rbeta(100000, 10, 1)
```


1. Mean = `r round(10/11,3)`, interval = (`r round(qbeta(.025,10,1),3)`,`r round(qbeta(.975,10,1),3)`)

2. Mean = `r round(mean(sims2b),3)`, interval = (`r round(quantile(sims2b, probs = c(.025, .975)),3)`)

3. Mean = `r round(mean(sims3b),3)`, interval = (`r round(quantile(sims3b, probs = c(.025, .975)),3)`)

4. Mean = `r round(mean(sims4b),3)`, interval = (`r round(quantile(sims4b, probs = c(.025, .975)),3)`)


### c
Beta(10,90)

```{r}
sims2c <- rbeta(1000, 10, 90)
sims3c <- rbeta(10000, 10, 90)
sims4c <- rbeta(100000, 10, 90)
```


1. Mean = `r round(10/100,3)`, interval = (`r round(qbeta(.025,10,90),3)`,`r round(qbeta(.975,10,90),3)`)

2. Mean = `r round(mean(sims2c),3)`, interval = (`r round(quantile(sims2c, probs = c(.025, .975)),3)`)

3. Mean = `r round(mean(sims3c),3)`, interval = (`r round(quantile(sims3c, probs = c(.025, .975)),3)`)

4. Mean = `r round(mean(sims4c),3)`, interval = (`r round(quantile(sims4c, probs = c(.025, .975)),3)`)

### d
Comment how effective the Monte Carlo tecniques are at estimating these properties of the distributions.

*It seems pretty good, especially when the number of samples increases.*