---
title: "Activity 11: key"
format: gfm
editor: source
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: FALSE
library(tidyverse)
library(rjags)
library(gridExtra)
```


### Last-Last Week's Recap

- MCMC algorithms


### This week

- posterior predictive distributions
- t-distributions and t-tests
- p-values
- Region of Practical Equivalence (ROPE)

---

### Bayesian modeling with t-distribution
- Sampling model $y \sim t(\mu, \sigma^2, \nu)$

- This requires a prior distribution on:
    - $\mu$: Similiar to the normal sampling model case, we can use a normal distribution with $p(\mu) \sim N(M,S^2)$ \vfill
    - $\sigma^2$: The variance term also has a similar interpretation, so we can use a uniform or inverse-gamma distribution for a prior. \vfill
    - $\nu$: The term $\nu$ is often called the degrees of freedom, and this controls the tail behavior of the distribution. The restriction is that the degrees of freedom has to be larger than one. A common prior is to use a shifted exponential distribution. 

```{r, warning=F, echo=F, fig.align='center', fig.width=6}
exp.samples <- data.frame(rexp(10000))
exp.samples2 <- data.frame(rexp(10000, rate = 10))
exp.samples3 <- data.frame(rexp(10000, rate = .1))
max.val <- max(max(cbind(exp.samples, exp.samples2, exp.samples3)))
colnames(exp.samples) <-colnames(exp.samples2) <-colnames(exp.samples3) <- 'vals'
plot1 <- ggplot(data=exp.samples, aes(vals)) + geom_histogram(bins = 100) + labs(subtitle = "rate = 1")
plot2  <- ggplot(data=exp.samples2, aes(vals)) + geom_histogram(bins = 100) + labs(subtitle = "rate = 10")
plot3 <- ggplot(data=exp.samples3, aes(vals)) + geom_histogram(bins = 100) + labs(subtitle = "rate = .1")
grid.arrange(plot2, plot1, plot3, nrow=3)
```


##### JAGS code

```{r, fig.align ='center', fig.width=5}
t.samples <- data.frame(rt(500, df = 3))
colnames(t.samples) <- 'vals'
ggplot(data=t.samples, aes(vals)) + geom_histogram(bins = 100) + 
  labs(subtitle = "samples from t(3) distribution")

#Prior parameters
M <- 0
S <- 100
C <- 10
rate <- .1

# Store data
dataList = list(y = t.samples$vals, Ntotal = nrow(t.samples), M = M, S = S, C = C, rate = rate)

# Model String
modelString = "model {
  for ( i in 1:Ntotal ) {
    y[i] ~ dt(mu, 1/sigma^2, nu) # sampling model
  }
  mu ~ dnorm(M,1/S^2)
  sigma ~ dunif(0,C)
  nu <- nuMinusOne + 1 # transform to guarantee n >= 1
  nuMinusOne ~ dexp(rate)
} "
writeLines( modelString, con='Tmodel.txt')

# initialization
initsList <- function(){
  # function for initializing starting place of theta
  # RETURNS: list with random start point for theta
  return(list(mu = rnorm(1, mean = M, sd = S), sigma = runif(1,0,C), 
              nuMinusOne = rexp(1, rate=rate) ))
}

# Runs JAGS Model
jagsT <- jags.model( file = "Tmodel.txt", data = dataList, inits =initsList, 
                     n.chains = 2, n.adapt = 1000)
update(jagsT, n.iter = 1000)

num.mcmc <- 1000
codaSamples <- coda.samples( jagsT, variable.names = c('mu', 'sigma','nu'), n.iter = num.mcmc)

par(mfcol=c(1,3))
traceplot(codaSamples)
densplot(codaSamples)
HPDinterval(codaSamples[[1]])
```


### 1. 
Simulate 100 responses from a Cauchy distribution, t distribution with $\mu$ = 1, $\sigma^2$=1 and $\nu=1$, and describe this data with a plot and brief description of the data.
```{r}
set.seed(03242025)
t.samples <- data.frame(rt(100, df = 1))
colnames(t.samples) <- 'vals'
ggplot(data=t.samples, aes(vals)) + geom_histogram(bins = 100) + 
  labs(subtitle = "samples from Cauchy distribution")
```
As can be seen from the quantiles of the data there are a few extreme observations, but most of the mass is fairly close to zero.

### 2. 
Use JAGS to fit a normal sampling model and the following priors for this data. 

 - $p(\mu) \sim N(0,10^2)$
 - $p(\sigma) \sim U(0,1000)$

Discuss the posterior HDIs for $\mu$ and $\sigma$.


```{r}
#Prior parameters
M <- 0
S <- 10
C <- 1000

# Store data
dataList = list(y = t.samples$vals, Ntotal = nrow(t.samples), M = M, S = S, C = C)

# Model String
modelString = "model {
  for ( i in 1:Ntotal ) {
    y[i] ~ dnorm(mu, 1/sigma^2) # sampling model
  }
  mu ~ dnorm(M,1/S^2)
  sigma ~ dunif(0,C)
} "
writeLines( modelString, con='NORMmodel.txt')

# initialization
initsList <- function(){
  # function for initializing starting place of theta
  # RETURNS: list with random start point for theta
  return(list(mu = rnorm(1, mean = M, sd = S), sigma = runif(1,0,C) ))
}

# Runs JAGS Model
jags.norm <- jags.model( file = "NORMmodel.txt", data = dataList, inits =initsList,
                         n.chains = 2, n.adapt = 1000)
update(jags.norm, n.iter = 1000)

num.mcmc <- 1000
coda.norm <- coda.samples( jags.norm, variable.names = c('mu', 'sigma'), n.iter = num.mcmc)

par(mfcol=c(2,2))
traceplot(coda.norm)
densplot(coda.norm)
HPDinterval(coda.norm[[1]])
```
The interval for $\mu$ is roughly centered around zero, but is fairly wide with a large degree of uncertainty. The interval for $\sigma$ is very large, compared to simulated variance of 1 from the t-distribution. 

### 3. 
Use JAGS to fit a t sampling model and the following priors for this data. 

 - $p(\mu) \sim N(0,10^2)$
 - $p(\sigma) \sim U(0,1000)$
 - $p(\nu) \sim E_+(.1)$, where $E_+(.1)$ is a shifted exponential with rate = .1.

Discuss the posterior HDIs for $\mu$, $\sigma$, and $\nu$.

```{r}
#Prior parameters
M <- 0
S <- 10
C <- 1000
rate <- .1

# Store data
dataList = list(y = t.samples$vals, Ntotal = nrow(t.samples), M = M, S = S, C = C, rate = rate)

# Model String
modelString = "model {
  for ( i in 1:Ntotal ) {
    y[i] ~ dt(mu, 1/sigma^2, nu) # sampling model
  }
  mu ~ dnorm(M,1/S^2)
  sigma ~ dunif(0,C)
  nu <- nuMinusOne + 1 # transform to guarantee n >= 1
  nuMinusOne ~ dexp(rate)
} "
writeLines( modelString, con='Tmodel.txt')

# initialization
initsList <- function(){
  # function for initializing starting place of theta
  # RETURNS: list with random start point for theta
  return(list(mu = rnorm(1, mean = M, sd = S), sigma = runif(1,0,C), 
              nuMinusOne = rexp(1, rate=rate) ))
}

# Runs JAGS Model
jagsT <- jags.model( file = "Tmodel.txt", data = dataList, inits =initsList, 
                     n.chains = 2, n.adapt = 1000)
update(jagsT, n.iter = 1000)

coda.t <- coda.samples( jagsT, variable.names = c('mu', 'sigma','nu'), n.iter = num.mcmc)

par(mfcol=c(1,3))
traceplot(coda.t)
densplot(coda.t)
HPDinterval(coda.t)
```
The intervals contain the true values with fairly low uncertainty. The only exception is that the $\nu$ value is larger than 1, but this is due to the prior specification and the fact that a t-distribution with $\nu < 1$ in not valid.

### 4. 
Create a data visualization to show posterior predictive distributions for Q2 and Q3. Remember this can be done using your posterior samples and combining them with your sampling model. Compare the data and the posterior predictive model curves with posterior predictive models. Note this is the final step in Bayesian data analysis: verifying that our model / prior selection is an accurate representation of the data. 

  
```{r}
# Posterior Predictive Normal
post.pred.normal <- rnorm(num.mcmc, coda.norm[[1]][,'mu'], coda.norm[[1]][,'sigma'] )
# Posterior Predictive t
post.pred.t <- rt(num.mcmc, df = coda.t[[1]][,'nu']) * coda.t[[1]][,'sigma'] + coda.t[[1]][,'mu']
data.comb <- data.frame(vals = c(t.samples$vals, post.pred.normal, post.pred.t), 
                        model = c(rep('data',100), rep('normal', num.mcmc), rep('t',num.mcmc)))

ggplot(data.comb, aes(vals, ..density.., colour = model)) + geom_freqpoly() +
  ggtitle('Comparison of Posterior Predictive Distributions') 
```


### T-test

For this question, we will use classical t-tests.

First write down the statistical model you are using for each of the following scenarios.

$height_i = \mu_i + \epsilon_i$, where $\epsilon_i \sim N(0, \sigma^2)$

Use the OK Cupid dataset and test the following claim, the mean height OK Cupid respondents reporting their body type as athletic is different than 70 inches. 

```{r}
okc <- read_csv('http://www.math.montana.edu/ahoegh/teaching/stat408/datasets/OKCupid_profiles_clean.csv')
okc.athletic <- okc %>% filter(body_type == 'athletic')

t.test(okc.athletic$height)
```


Now consider whether there is a  height difference between  OK Cupid respondents self-reporting their body type as "athletic" and those self-reporting their body type as "fit"



```{r}
okc.fit <- okc %>% filter(body_type == 'fit')

t.test(okc.athletic$height, okc.fit$height)
```

