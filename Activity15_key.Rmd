---
title: "Week 15 Activity"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(rjags)
library(runjags)
library(tidyverse)
library(arm)
set.seed(04262023)
knitr::opts_chunk$set(echo = TRUE)
```


For this activity we will use NBA free throw shooting data.

```{r, echo = F}
playoff.free.throws <- tibble(
  player = c('James','Curry','Irving','Durant','Wall','Leonard','Beal','Harden','Love','Horford','Aldridge','Green','Paul','Olynyk','Parker','Favors','Jordan','Oladipo'),
  position = c('F','G','G','F','G','G','G','G','F','F','F','F','G','F','G','F','F','G'),
  FTA = c(162,114,84,103,93,102,61,115,75,29,55,67,33,30,14,23,56,6),
  FTM = c(113,103,76,92,78,95,50,101,63,22,42,46,29,22,14,11,22,6)) %>% 
  arrange(position, player)
playoff.free.throws %>% kable(digits = 2)
```

#### JAGS Code for Hierarchical Model
```{r, echo = F, message = F}
# Data
guards <- playoff.free.throws %>% filter(position == 'G')
Nsubj <- nrow(guards)
dataList = list(z=guards$FTM, N=guards$FTA,Nsubj = Nsubj)
```

```{r}
# Model - see page 250 in text
modelString <- 'model {
  for ( s in 1:Nsubj ) {
    z[s] ~ dbin( theta[s], N[s] )
    theta[s] ~ dbeta( omega*(kappa-2)+1, (1-omega)*(kappa-2)+1)
  }
  omega ~ dbeta(2.55, .45) # 85%
  kappa <- kappaMinusTwo + 2
  kappaMinusTwo ~ dgamma(.01, .01)
}'
writeLines( modelString, con='HierModel.txt')


# RUN JAGS
jags.hier <- jags.model( file = "HierModel.txt", 
                         data = dataList, 
                         n.chains = 3, 
                         n.adapt = 50000)
update(jags.hier, 10000)

num.mcmc <- 10000
codaSamples <- coda.samples( jags.hier, 
                             variable.names = c('omega', 'kappa','theta'), 
                             n.iter = num.mcmc)
```

```{r, echo = F}
guards <- guards %>% mutate(FT.PCT  = FTM / FTA)
cbind(guards, 
      post.mean = colMeans((combine.mcmc(codaSamples))[,-c(1:2)]),
                           HPDinterval(combine.mcmc(codaSamples))[-c(1:2),]) %>% 
  kable(digits = 2)
```

The mean and HPD values for omega and kappa are

```{r}
HPDinterval(combine.mcmc(codaSamples))[1:2,]
colMeans((combine.mcmc(codaSamples))[,c(1:2)])

```



## Exercises

1. Specify independent models using a uniform prior for $\theta$ for the players Curry, Beal, and Oladipo. Write the the sampling model and priors for these models. Recall you can find the posterior here analytically without using MCMC.

$$FTM_i \sim Binomial(FTA_i, \theta_i)$$
where $i$ denotes the $i^{th}$ player and $\theta_i$ is the free throw shooting percentage for player i.

$\theta_i \sim Beta(1,1)$ for all i.

\vfill

2. Specify independent models using an informative prior, of your choice, for $\theta$ for the players Curry, Beal, and Oladipo. Write the the sampling model and priors for these models. Recall you can find the posterior here analytically without using MCMC. Defend your prior choice.
\vfill

$$FTM_i \sim Binomial(FTA_i, \theta_i)$$
where $i$ denotes the $i^{th}$ player and $\theta_i$ is the free throw shooting percentage for player i.

$\theta_i \sim Beta(25.5,4.5)$ for all i. This corresponds to roughly 85% free throw shooting and is weighted equivalently to taking 30 shots (and making 25.5).

3. Compare the posterior HDI from these models with those found using the hierarchical models. Note the HPDinterval function can be applied directly to samples from a beta distribution as `HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = a.star, shape2 = b.star)))`


#### Uniform Priors
- Beal (50 / 61 = `r round(50/61, 2)`) Posterior mean is `r round((50 + 1)/(50 + 1 + 11 + 1),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 50 + 1, shape2 = 11 + 1))),2)` )

- Curry (103/ 114 = `r round(103/114, 2)`) Posterior mean is `r round((103 + 1)/(103 + 1 + 11 + 1),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 103 + 1, shape2 = 11 + 1))),2)` )

- Oladipo (6/ 6 = `r round(6/6, 2)`) Posterior mean is `r round((6 + 1)/(6 + 1 + 0 + 1),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 6 + 1, shape2 = 0 + 1))),2)` )


#### Informative Priors
- Beal (50 / 61 = `r round(50/61, 2)`) Posterior mean is `r round((50 + 25.5)/(50 + 25.5 + 11 + 4.5),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 50 + 25.5, shape2 = 11 + 4.5))),2)`) 

- Curry (103/ 114 = `r round(103/114, 2)`) Posterior mean is `r round((103 + 25.5)/(103 + 25.5 + 11 + 4.5),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 103 + 25.5, shape2 = 11 + 4.5))),2)` )

- Oladipo (6/ 6 = `r round(6/6, 2)`) Posterior mean is `r round((6 + 25.5)/(6 + 25.5 + 0 + 4.5),2)`
HPD is (`r round(HPDinterval(mcmc(data  = rbeta(n = 5000, shape1 = 6 + 25.5, shape2 = 0 + 4.5))),2)`)

\vfill

4. Reflect on the differences/similarities in the credible intervals between the two different priors as well as the hierarchical model. If you were going to bet on the players shooting percentages for the next season, which would you prefer?

**The differences are not as large as I'd expect, particularly for Oladipo. Some of that is that the uniform prior effectively shrinks toward .5 and the point estimate ends up being $\frac{6+1}{6 + 1 +1} = \frac{7}{8}$, which is about .88 and quite close to my prior mean. I do think I prefer the informative priors in this case; however, the HM effectively serves the role as a "good" informative prior as that value is calculated from the data.**

\vfill

5. Now fit a frequentist model to estimate $\theta$ for the players Curry, Beal, and Oladipo. Describe the sampling model you've assumed with your code (hint: `glm()` is one possible route). Discuss the differences in your intervals with what you've computed in part 3. If you had to choose one analysis: HM, Bayes with uniform priors, Bayes with informative priors, or the frequentist approach which would you choose and why? 
\vfill

**We use the same model as parts 1 & 2 (sans the prior, of course).

```{r}
beal <- glm(cbind(50,11)~ 1,  family = "binomial")
curry <- glm(cbind(103,11)~ 1,  family = "binomial")
oladipo <- glm(cbind(6,0)~ 1,  family = "binomial")
```

- Beal (50 / 61 = `r round(50/61, 2)`) MLE is `r round(invlogit(coef(beal)),2)`
CI is (`r round(invlogit(confint(beal)),2)`) 

- Curry (103/ 114 = `r round(103/114, 2)`) MLE is `r round(invlogit(coef(curry)),2)`
CI is (`r round(invlogit(confint(curry)),2)`) 

- Oladipo (6/ 6 = `r round(6/6, 2)`) MLE is `r round(invlogit(coef(oladipo)),2)`
CI is (`r round(invlogit(confint(oladipo)),2)`) 

**The Oladipo case is obviously troubling here, but in general the philosophy of sharing information from hierarchical models seems reasonable in this setting.**

\vfill
\vfill
\newpage

## Optional Exercise

Use a dataset containing baseball batting averages and fit a hierarchical model with the groups as the player positions. Your goal is to model batting averages of the players and assess the differences between the player positions in this dataset.
```{r}
BattingAverage <- read.csv('http://math.montana.edu/ahoegh/teaching/stat491/data/BattingAverage.csv')
head(BattingAverage)
```

Assume this is an analysis you have been asked to perform as part of a job interview. Write a 1-2 page summary of your analysis using R Markdown. This should include an introduction and conclusion.

```{r, eval=F}
# Data
z <- BattingAverage$Hits
N <- BattingAverage$AtBats
s <- BattingAverage$PlayerNumber
c <- BattingAverage$PriPosNumber
Nsubj = length(unique(s))
Ncat =  length(unique(c))

dataList = list(
    z = z ,
    N = N ,
    c = as.numeric(c) , # c in JAGS is numeric, in R is possibly factor
    Nsubj = Nsubj ,
    Ncat = Ncat
  )

# Model
modelString = "
  model {
    for ( sIdx in 1:Nsubj ) {
      z[sIdx] ~ dbin( theta[sIdx] , N[sIdx] )
      theta[sIdx] ~ dbeta( omega[c[sIdx]]*(kappa[c[sIdx]]-2)+1 , 
                           (1-omega[c[sIdx]])*(kappa[c[sIdx]]-2)+1 ) 
    }
    for ( cIdx in 1:Ncat ) {
      omega[cIdx] ~ dbeta( omegaO*(kappaO-2)+1 , 
                           (1-omegaO)*(kappaO-2)+1 )
      kappa[cIdx] <- kappaMinusTwo[cIdx] + 2
      kappaMinusTwo[cIdx] ~ dgamma( 0.01 , 0.01 ) # mean=1 , sd=10 (generic vague)
    }
    omegaO ~ dbeta( 1.0 , 1.0 ) 
    kappaO <- kappaMinusTwoO + 2
    kappaMinusTwoO ~ dgamma( 0.01 , 0.01 )  # mean=1 , sd=10 (generic vague)
  }
  " 
writeLines( modelString, con='HierModelComb.txt')

# Initialize
initsList = function() {
    thetaInit = rep(NA,Nsubj)
    for ( sIdx in 1:Nsubj ) { # for each subject
      resampledZ = rbinom(1, size=N[sIdx] , prob=z[sIdx]/N[sIdx] )
      thetaInit[sIdx] = resampledZ/N[sIdx]
    }
    thetaInit = 0.001+0.998*thetaInit # keep away from 0,1    
    kappaInit = 100 # lazy, start high and let burn-in find better value
    return( list( theta=thetaInit , 
                  omega=aggregate(thetaInit,by=list(c),FUN=mean)$x ,
                  omegaO=mean(thetaInit) ,
                  kappaMinusTwo=rep(kappaInit-2,Ncat) ,
                  kappaMinusTwoO=kappaInit-2 ) )
}

 # RUN THE CHAINS
  parameters = c( "theta","omega","kappa","omegaO","kappaO") 
  adaptSteps = 10000             # Number of steps to adapt the samplers
  burnInSteps = 10000            # Number of steps to burn-in the chains
  nChains = 2
  num.mcmc = 50000

# MCMC
jagsModel = jags.model( "HierModelComb.txt" , data=dataList , inits=initsList , 
                            n.chains=nChains , n.adapt=adaptSteps )
    # Burn-in:
    cat( "Burning in the MCMC chain...\n" )
    update( jagsModel , n.iter=burnInSteps )
    # The saved MCMC chain:
    cat( "Sampling final MCMC chain...\n" )
    codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                                n.iter=num.mcmc)

HPDinterval(codaSamples)
```

